{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Vanilla Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image # To grab the images and extract useful information\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "np.random.seed(42) # Set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the labels and file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4870, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the dataset directory\n",
    "dataset_dir = os.getcwd() + \"/train_selected\"\n",
    "\n",
    "# Get the data labels\n",
    "labels_file = dataset_dir + \"/train_selected.csv\"\n",
    "data_labels = pd.read_csv(labels_file)\n",
    "\n",
    "data_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X files\n",
    "file_list = [dataset_dir + \"/\" + str(x) + \".png\" for x in list(data_labels[\"id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4370\n",
       "1     500\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the labels\n",
    "data_labels[\"class\"] = np.where(data_labels['label']=='automobile', 1, 0)\n",
    "data_labels[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will standardise the dataset\n",
    "# Replace False\n",
    "\n",
    "def standarise_data(dataset):\n",
    "    \n",
    "    new_dataset = dataset/255.\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global X_train, X_test, y_train, y_test, X, y\n",
    "    \n",
    "    X = np.array([np.array(Image.open(fname)) for fname in file_list])\n",
    "    y = np.array(data_labels[\"class\"])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    y_train = y_train.reshape(1, y_train.shape[0])\n",
    "    y_test = y_test.reshape(1, y_test.shape[0])\n",
    "    \n",
    "    # Reshape the training and test examples \n",
    "    X_train_f = X_train.reshape(X_train.shape[0], -1).T\n",
    "    X_test_f = X_test.reshape(X_test.shape[0], -1).T\n",
    "    \n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    X_train = standarise_data(X_train_f)\n",
    "    X_test = standarise_data(X_test_f)\n",
    "    \n",
    "\n",
    "    print (\"Flatten X_train: \" + str(X_train.shape))\n",
    "    print (\"Flatten X_test: \" + str(X_test.shape))\n",
    "    \n",
    "    print (\"y_train: \" + str(y_train.shape))\n",
    "    print (\"y_test: \" + str(y_test.shape))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten X_train: (3072, 3409)\n",
      "Flatten X_test: (3072, 1461)\n",
      "y_train: (1, 3409)\n",
      "y_test: (1, 1461)\n"
     ]
    }
   ],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick 'normal' ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the following resources useful:\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3409, 3072) (1461, 3072) (3409,) (1461,)\n"
     ]
    }
   ],
   "source": [
    "X_train_clf = X_train.T\n",
    "X_test_clf = X_test.T\n",
    "\n",
    "y_train_clf = y_train.T.ravel()\n",
    "y_test_clf = y_test.T.ravel()\n",
    "\n",
    "print(X_train_clf.shape, X_test_clf.shape, y_train_clf.shape, y_test_clf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4607719ad1f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Fit to our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total time taken: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1685\u001b[0m                       \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m                       )\n\u001b[0;32m-> 1687\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_encoded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m             for train, test in folds)\n\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "import datetime\n",
    "\n",
    "C_list = np.linspace(0.001, 0.5, 20)\n",
    "log_reg = LogisticRegressionCV(\n",
    "    Cs=C_list, cv=10, penalty='l2', scoring='roc_auc', solver='liblinear', tol =1e-4, max_iter=1000, \n",
    "    class_weight='balanced', n_jobs=7, verbose=2, refit=True, multi_class='ovr', random_state=42\n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "log_reg.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the class\n",
    "y_test_clf = pd.DataFrame(y_test_clf, columns=[\"actual\"])\n",
    "y_test_clf[\"predictions_lr\"] = log_reg.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion Matrix \n",
      " [[1154  143]\n",
      " [  53  111]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.89      0.92      1297\n",
      "          1       0.44      0.68      0.53       164\n",
      "\n",
      "avg / total       0.90      0.87      0.88      1461\n",
      "\n",
      "ROC-AUC Score \n",
      " 0.7832874174925251\n",
      "Accuracy Score \n",
      " 0.865845311430527\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_lr))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=7)]: Done 786 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=7)]: Done 1000 out of 1000 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 0:00:18.865685\n"
     ]
    }
   ],
   "source": [
    "# A tree based example\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "\n",
    "#Create the model object\n",
    "rf_class = RandomForestClassifier(\n",
    "    n_estimators=1000, criterion='entropy', \n",
    "    max_depth=15, min_samples_split=3, bootstrap=True, oob_score=True, \n",
    "    n_jobs=7, random_state=42, verbose=1, class_weight='balanced' \n",
    ")\n",
    "\n",
    "#Fit to our model\n",
    "start = datetime.datetime.now()\n",
    "rf_class.fit(X_train_clf, y_train_clf)\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 786 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=7)]: Done 1000 out of 1000 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#Predict the class\n",
    "y_test_clf[\"predictions_rf\"] = rf_class.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confustion Matrix \n",
      " [[1297    0]\n",
      " [ 151   13]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.94      1297\n",
      "          1       1.00      0.08      0.15       164\n",
      "\n",
      "avg / total       0.91      0.90      0.86      1461\n",
      "\n",
      "ROC-AUC Score \n",
      " 0.5396341463414634\n",
      "Accuracy Score \n",
      " 0.8966461327857632\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get classification report\n",
    "print(classification_report(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get ROC-AUC\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test_clf.actual, y_test_clf.predictions_rf))\n",
    "\n",
    "# Get accuracy\n",
    "print(\"Accuracy Score \\n\", accuracy_score(y_test_clf.actual, y_test_clf.predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3072, 10, 25, 10, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Correctly create the layer dimensions as per the brief\n",
    "# Replace False\n",
    "\n",
    "# For example layer_dimensions of [5,7,2,1] would be input 5, two hidden layers (7,2) and 1 in output\n",
    "\n",
    "layer_dimensions = [3072,10,25,10,1]\n",
    "layer_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Use your knowledge of parameter matrix size to edit the code. \n",
    "# Replace False\n",
    "\n",
    "def initialise_parameters(layer_dimensions):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    layer_dimensions -- python (list), one item per layer, number representing size of layer\n",
    "    \n",
    "    Output:\n",
    "    parameters -- python dictionary containing your weight and bias parameters \"W1\", \"b1\", ..., \"WL\", \"bL\" \n",
    "                  with appropriate sizes.\n",
    "    \"\"\"\n",
    "    \n",
    "    global parameters\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dimensions)         \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l-1]) * 0.01 \n",
    "        parameters['b' + str(l)] = np.zeros((layer_dimensions[l],1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.00496714 -0.00138264  0.00647689 ...  0.00692723 -0.0126933\n",
      "   0.01702515]\n",
      " [ 0.00202329  0.01631857 -0.00733033 ... -0.00045929 -0.00016568\n",
      "   0.00683325]\n",
      " [ 0.00590744 -0.00220114  0.00063649 ...  0.00532892 -0.00617286\n",
      "   0.01202249]\n",
      " ...\n",
      " [-0.00046009  0.00611291  0.0023457  ...  0.00043588  0.0051953\n",
      "  -0.00264349]\n",
      " [ 0.00516406 -0.00085402  0.01011368 ...  0.00567622 -0.01582958\n",
      "  -0.00766962]\n",
      " [ 0.00289715 -0.01204151 -0.0066031  ... -0.01609538  0.01883204\n",
      "  -0.00683668]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 1.41266780e-02 -2.21370910e-03 -1.13080420e-02  5.09068390e-03\n",
      "  -3.91056857e-03 -2.07101106e-03  1.44905934e-03 -8.12212836e-03\n",
      "  -1.62113635e-02 -5.12696147e-04]\n",
      " [ 9.63917703e-03 -9.01591259e-03  1.53962266e-03  1.50550715e-02\n",
      "  -8.97648873e-04  8.60943718e-04  5.52123536e-04 -6.82344426e-03\n",
      "  -9.93354518e-03 -9.89262369e-03]\n",
      " [ 3.68183197e-03  7.57872362e-03  8.05082238e-03 -3.78542854e-03\n",
      "   1.09330854e-02  3.07363820e-03  7.07422218e-04 -3.33935666e-02\n",
      "  -8.17409612e-03 -1.52109790e-02]\n",
      " [ 6.27073507e-03  9.50333319e-04 -1.07198639e-02  3.04362558e-03\n",
      "  -1.42349237e-02 -1.04376310e-02 -7.10221331e-03  5.40136322e-03\n",
      "   1.39944168e-02 -5.86008448e-03]\n",
      " [ 1.32245864e-02  4.74035506e-04  2.36138161e-03 -6.38045577e-04\n",
      "  -1.97313753e-03  4.39300037e-03  7.40321186e-03 -8.84611304e-03\n",
      "  -1.89300470e-03  1.66927510e-02]\n",
      " [ 1.77188654e-03  2.86693854e-03  1.16726854e-02 -5.01244499e-03\n",
      "  -3.39606219e-04  8.16844897e-03  1.23890223e-02 -2.36514329e-02\n",
      "  -7.36668791e-03 -9.98365306e-03]\n",
      " [-1.23825103e-03 -9.22672777e-03  6.64105500e-03  1.72410341e-03\n",
      "   8.89190169e-03  9.53007957e-03  6.79086018e-03 -3.35790551e-03\n",
      "   1.04277539e-03 -4.60873905e-03]\n",
      " [-1.36473531e-02  9.53787847e-03 -8.40646273e-04 -1.50261196e-02\n",
      "   5.36949571e-03 -2.23948918e-03 -2.05980397e-02 -1.37372927e-02\n",
      "  -7.00611159e-03  2.10512058e-03]\n",
      " [ 7.88019985e-03  7.53768931e-03  6.02862955e-03 -3.43623717e-04\n",
      "   1.72223231e-02  6.99871244e-03  2.22105369e-03  5.66588106e-03\n",
      "  -3.99035003e-03 -5.72436010e-04]\n",
      " [-2.08195404e-02 -3.86747662e-03  1.75988802e-02  1.48375277e-03\n",
      "   1.06739777e-03  9.98604095e-03  3.57825416e-03 -7.86483804e-03\n",
      "   2.36575122e-04 -1.13521912e-02]\n",
      " [ 1.83612171e-03 -8.37749952e-03  9.28294612e-03 -5.10240622e-03\n",
      "  -8.24703909e-03 -1.12831988e-03  3.65391819e-03 -1.20733200e-02\n",
      "  -5.21266597e-03  1.05162306e-03]\n",
      " [ 1.33836855e-02 -2.52871576e-02  6.15163204e-03 -3.83565564e-03\n",
      "  -4.94610347e-03 -6.81126960e-03 -3.63895584e-03  6.20817201e-03\n",
      "  -5.36786471e-03  1.65859202e-02]\n",
      " [ 6.23866784e-03  1.97972125e-03 -1.66056118e-03  1.18985420e-02\n",
      "   1.33601969e-02 -1.16138215e-02 -7.01898105e-03  6.34630206e-03\n",
      "   9.36680944e-03 -3.00340548e-03]\n",
      " [-7.42444860e-03  7.86126260e-04  1.47669343e-02  4.10342601e-04\n",
      "  -1.26394077e-02  1.09583228e-03  7.82748071e-03 -1.01377311e-03\n",
      "   1.25816681e-02  8.65849036e-03]\n",
      " [-6.17589237e-04  4.31440196e-04 -1.84175971e-02  1.21022672e-02\n",
      "  -3.29517349e-03 -3.99956739e-03 -5.49853607e-03  4.56697562e-03\n",
      "  -1.12869300e-02 -9.84172782e-03]\n",
      " [-5.25251017e-04 -6.26513332e-03  3.35233573e-03 -5.89889279e-03\n",
      "   1.59503626e-02  6.93457507e-03  6.80886212e-03 -6.04961754e-03\n",
      "   2.40567967e-02 -4.39011014e-03]\n",
      " [-7.13810277e-03  7.06742597e-03  1.95228133e-03 -8.18384309e-03\n",
      "   2.19584716e-02  9.75599498e-03  4.00530127e-05  2.40111013e-02\n",
      "   8.10780144e-03 -1.19009698e-04]\n",
      " [ 2.28143765e-02  6.58984733e-03  6.15245262e-03 -3.86347627e-03\n",
      "   4.94134857e-04 -4.10692996e-03  1.43973575e-02 -1.60941414e-02\n",
      "   1.36587043e-02 -8.77342529e-03]\n",
      " [-5.79212032e-04 -2.95216388e-03 -5.20152486e-03  6.89784148e-03\n",
      "  -5.18921333e-05 -6.67698760e-03  1.53995530e-03 -6.96086727e-03\n",
      "  -5.37775499e-03 -1.49726309e-02]\n",
      " [-4.50308993e-03  4.94204526e-03  3.71249180e-04  1.81506367e-02\n",
      "   2.61281747e-02 -4.64218396e-03 -8.33317758e-03  1.12981655e-02\n",
      "   3.35208242e-03  1.13587428e-02]\n",
      " [ 3.98830731e-03 -1.94357847e-03 -1.88685048e-03  4.32776778e-03\n",
      "  -2.11113896e-03  1.21950124e-02 -1.30233724e-02  1.21414364e-02\n",
      "  -1.87340075e-02  1.45102286e-03]\n",
      " [-3.31042731e-03 -1.51635570e-02 -2.06001942e-02 -1.09751467e-02\n",
      "   4.56843266e-04 -6.02184129e-03 -1.04094890e-02  1.35228108e-02\n",
      "  -1.23218468e-02  6.79117332e-03]\n",
      " [-8.88997008e-03 -1.12280593e-02 -6.03838627e-03  6.19916047e-03\n",
      "  -1.37146576e-02  2.87455261e-03 -9.60759998e-03 -7.80766870e-03\n",
      "   4.00641589e-03  2.17348283e-02]\n",
      " [-9.11016990e-03  7.19880584e-03 -3.12712377e-03 -4.74743455e-03\n",
      "   1.07137601e-02 -8.43522811e-03 -1.15411323e-02  4.60175795e-03\n",
      "   1.59478380e-02  7.49994705e-03]\n",
      " [-1.40701738e-02 -6.15333044e-03  9.79598242e-03  1.58111022e-02\n",
      "   2.28711325e-03 -5.79172534e-03  1.64601884e-02 -7.45935852e-04\n",
      "  -6.05808574e-03 -7.74932538e-03]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# It would be a good idea to test your initialisation function here. These matrix sizes are important!\n",
    "\n",
    "parameters = initialise_parameters((layer_dimensions))\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward prop\n",
    "\n",
    "Create a series of functions that will:\n",
    "\n",
    "* Undertake the linear multiplication\n",
    "* Underake the activation of the layer\n",
    "* Store this somewhere for efficient computation of backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations\n",
    "\n",
    "We will need activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will undertake sigmoid activaiton\n",
    "# Create another function that will undertake relu activation\n",
    "# Replace False\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of sigmoid(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "        \n",
    "    A = 1/(1+np.exp(-Z)) #Added Sigmoid\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    Z     -- numpy array of any shape\n",
    "    \n",
    "    Output:\n",
    "    A     -- output of relu(z), (should be same shape as Z!)\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    cache = Z \n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create a function that will undertake the linear component of forward prop\n",
    "# Replace False\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    A     -- activations from previous layer\n",
    "    W     -- weights matrix\n",
    "    b     -- bias vector\n",
    "\n",
    "    Output:\n",
    "    Z     -- the input to activation function \n",
    "    cache -- a python dictionary with \"A\", \"W\" and \"b\" for backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b #multply the activations by the weights and add the bias\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# This function conditionally calls an activation function. \n",
    "# Call the correction function above with the correct if statement\n",
    "# Replace False\n",
    "\n",
    "def activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "    A_prev     -- activations from previous layer\n",
    "    W          -- weights matrix\n",
    "    b          -- bias vector\n",
    "    activation -- the activation type to be used (\"sigmoid\" or \"relu\")\n",
    "\n",
    "    Output:\n",
    "    A          -- the output of the activation function, also called the post-activation value \n",
    "    cache      -- a python dictionary with two two caches \"linear_cache\" and \"activation_cache\" for backprop\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ###NOTE###\n",
    "    # This is where you can put more activation functions for the extension tasks\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "\n",
    "    return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Architect the forward pass. \n",
    "# You will need to firstly determine how many layers there are\n",
    "# You will then need to pull out the correct parameters we initalised\n",
    "# Ensure you use the appropriate activation for the middle layers\n",
    "# Pay special attention to the last layer\n",
    "# It may help to print out parameters\n",
    "# Replace False\n",
    "\n",
    "def total_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    X            -- raw data\n",
    "    parameters   -- dictionary of initialised parameters, output from a particular function above.\n",
    "    \n",
    "    Returns:\n",
    "    AL           -- last post-activation value\n",
    "    caches       -- list of caches from forward activations\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(layer_dimensions) - 1\n",
    "    \n",
    "    # All the layers up until the last (sigmoid) layer\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = activation_forward(A_prev, \n",
    "                                      parameters['W' + str(l)], \n",
    "                                      parameters['b' + str(l)],\n",
    "                                       activation='relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # The last layer - how do we use the sigmoid function?\n",
    "    \n",
    "    AL, cache = activation_forward(A, \n",
    "                                   parameters['W' + str(l+1)], \n",
    "                                   parameters['b' + str(l+1)],  \n",
    "                                   activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Differentiate the relu and the sigmoid functions\n",
    "# Replace False\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in the backwards prop here.\n",
    "\n",
    "    Output:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.copy(dA) # Copying dA first\n",
    "    \n",
    "    # What do you set dZ to when Z is what values? \n",
    "    dZ[Z <= 0] = 0;\n",
    "    \n",
    "    assert (dZ.shape == Z.shape);\n",
    "        \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA      -- post-activation gradient\n",
    "    cache   -- 'Z' that is used in backprop here\n",
    "\n",
    "    Returns:\n",
    "    dZ      -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    " \n",
    "    dZ = dA * Z * (1 - Z)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "        \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to do anything here, but notes are included for your interest\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dZ        -- Gradient of the cost with respect to 'Z' of current layer\n",
    "    cache     -- (A_prev, W, b) from forward propag in the current layer, we stored this previously\n",
    "\n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Use the activation differentiation functions you created above\n",
    "# Ensure you are putting the right arguments (hint: caches) into the functions\n",
    "# For the first false, consider what function give back dZ? (what does it require?)\n",
    "# Replace False\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dA         -- post-activation gradient for current layer\n",
    "    cache      -- (linear_cache, activation_cache) stored previously for backprop\n",
    "    activation -- activation for this layer (\"sigmoid\" or \"relu\")\n",
    "    \n",
    "    Output:\n",
    "    dA_prev   -- Gradient of the cost w.r.t activation of previous layer\n",
    "    dW        -- Gradient of the cost w.r.t W of current layer\n",
    "    db        -- Gradient of the cost w.r.t b of current layer l\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Differentiate the loss function with respect to the last activation layer\n",
    "# Replace False\n",
    "\n",
    "def total_backward(AL, Y, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    AL        -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y         -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches    -- list of caches from relu and sigmoid we kept from forward prop\n",
    "    \n",
    "    output:\n",
    "    grads     -- A dictionary with the gradients named dA+l,dW+l, db+l for each layer\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(layer_dimensions) - 1 \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Write a function to compute the binary logistic cost function ('cross entropy loss')\n",
    "# This is on page 51 of the slides from block_1. \n",
    "# You may need to transpose elements to make the matrix calculations work\n",
    "# Replace False\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    AL    -- probability vector for label predictions\n",
    "    Y     -- truth vector vector\n",
    "\n",
    "    Output:\n",
    "    cost  -- cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost_total =  (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "                     \n",
    "    cost = (1./m) * cost_total \n",
    "    \n",
    "    cost = np.squeeze(cost) # Help with the shape\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Update each parameter\n",
    "# Remember what hyperparameter is important for this step?\n",
    "# You will also find a useful, indexed value in the 'grads' dictionary created in backprop above\n",
    "# Replace False\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    \n",
    "    Input:\n",
    "    parameters    -- dictionary with parameters \n",
    "    grads         -- dictionary with gradients (which function outputs this?)\n",
    "    learning_date -- step size to adjust parameters by\n",
    "    \n",
    "    Returns:\n",
    "    parameters    -- dictionary containing your updated parameters , same structure as original parameters dict\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+ str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+ str(l+1)]\n",
    "        \n",
    "    return parameters, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to knit together everything you have done so far and allow for different layer sizes and lengths to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Now stitch it all together. Essentially you will need to call all your functions in turn with the right arguments.\n",
    "# Initialise parameters\n",
    "# Undertake forward prop. What is our master function? Consider what we got from initialisation?\n",
    "# Undertake backwards prop. Again consider our master function for back prop.\n",
    "# Update parameters.\n",
    "# Replace False\n",
    "\n",
    "def total_backward_forward(X, Y, layers_dimensions,  \n",
    "                           num_iterations, \n",
    "                           print_cost):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    X                 -- data\n",
    "    Y                 -- truth vector (1,0)'s\n",
    "    layers_dimensions -- list of dimensions for each layer of network\n",
    "    learning_rate     -- step size for gradient descent\n",
    "    num_iterations    -- number of training iterations to undertake\n",
    "    print_cost        -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    output:\n",
    "    parameters        -- parameters learnt by the model. Used to predict\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialise_parameters(layer_dimensions)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation:\n",
    "        AL, caches = total_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = total_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, 0.1)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: -0.693151\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9ba62e9cbb0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                     \u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                     print_cost = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-a355dd720678>\u001b[0m in \u001b[0;36mtotal_backward_forward\u001b[0;34m(X, Y, layers_dimensions, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Forward propagation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Compute cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-945266e65ad6>\u001b[0m in \u001b[0;36mtotal_forward\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         A, cache = activation_forward(A_prev, \n\u001b[0;32m---> 31\u001b[0;31m                                       \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                                       \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                        activation='relu')\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "parameters = total_backward_forward(X_train, \n",
    "                                    y_train, \n",
    "                                    layer_dimensions, \n",
    "                                    num_iterations = 1500, \n",
    "                                    print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict (Hold out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO ##\n",
    "\n",
    "# Create your own predict function.\n",
    "# Note the number of training examples\n",
    "# Turn the probabilities into 0-1 predictions\n",
    "# Replace False\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    X           -- data (test set)\n",
    "    parameters  -- parameters of the trained model\n",
    "    \n",
    "    Output:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # How many training examples?\n",
    "    n = len(parameters) // 2\n",
    "    p = np.zeros((1,m)) # Initialise probabilities to zero\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = total_forward(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions. \n",
    "    p = (probas < 0.5).astype(np.int)\n",
    "        \n",
    "    return p, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b9f878885065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create some predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-4d02a4c561ba>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X, y, parameters)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# How many training examples?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Initialise probabilities to zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "# Create some predictions\n",
    "predictions, probas = predict(X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatter plot of probabilities. Good check if something is wrong\n",
    "plt.scatter(range(len(probas[0])), probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your prediction value counts\n",
    "pred_df = pd.DataFrame(predictions, columns=[\"prediction\"])\n",
    "pred_df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a bit of reshaping\n",
    "predictions_sk = predictions.reshape(len(predictions), 1)\n",
    "print(predictions_sk.shape)\n",
    "\n",
    "y_test_sk = y_test.T\n",
    "print(y_test_sk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build some sklearn scores\n",
    "\n",
    "#Get confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(list(y_test_sk), list(predictions_sk)))\n",
    "\n",
    "#Get classification report\n",
    "print(classification_report(y_test_sk, predictions_sk))\n",
    "\n",
    "# Accuracy score\n",
    "print(\"Accuracy: \", accuracy_score(y_test_sk, predictions_sk))\n",
    "\n",
    "# ROC_AUC score\n",
    "print(\"ROC_AUC: \", roc_auc_score(y_test_sk, probas.T))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
